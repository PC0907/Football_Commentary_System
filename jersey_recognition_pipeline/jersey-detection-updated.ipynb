{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9855109,"sourceType":"datasetVersion","datasetId":6047705}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setting up the dependencies","metadata":{}},{"cell_type":"code","source":"pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.4/index.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"mmdet>=3.0.0\"  # Latest 3.x version\n!pip install \"mmpose>=1.2.0\"  # Latest 1.x version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip index versions mmcv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    import mmcv\n    print(\"MMCV version:\", mmcv.__version__)\n\n    import mmengine\n    print(\"MMEngine version:\", mmengine.__version__)\n\n    import mmdet\n    print(\"MMDet version:\", mmdet.__version__)\n\n    import mmpose\n    print(\"MMPose version:\", mmpose.__version__)\n\n    print(\"\\nAll required packages installed successfully!\")\nexcept ImportError as e:\n    print(\"Error:\", e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cloning the appropriate repositories","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Mostafa-Nafie/Football-Object-Detection","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/mkoshkina/jersey-number-pipeline/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/ViTAE-Transformer/ViTPose","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Now trying ViTPose dependency installations","metadata":{}},{"cell_type":"code","source":"# Set CUDA environment variables\n# import os\n# os.environ['CUDA_HOME'] = '/usr/local/cuda-12.3'\n# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-12.3/lib64'\n\n# Import the necessary modules\nfrom mmpose.apis import init_model, inference_topdown\nfrom mmpose.utils import register_all_modules\n\n# Verify PyTorch CUDA setup\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n\n# Print MM* versions\nimport mmcv\nimport mmengine\nimport mmdet\nimport mmpose\n\nprint(\"\\nInstalled versions:\")\nprint(f\"MMCV version: {mmcv.__version__}\")\nprint(f\"MMEngine version: {mmengine.__version__}\")\nprint(f\"MMDet version: {mmdet.__version__}\")\nprint(f\"MMPose version: {mmpose.__version__}\")\n\n# Try to register modules\nregister_all_modules()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setting up","metadata":{}},{"cell_type":"markdown","source":"#### Extracting frames and using legitablity classifier as a filter","metadata":{}},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\n\ndef create_folder_structure():\n    \"\"\"Create folder for storing detected frames\"\"\"\n    base_folder = 'Jersey_Detection_Project'\n    frames_folder = 'detected_frames'\n\n    # Create folders\n    os.makedirs(base_folder, exist_ok=True)\n    frames_path = os.path.join(base_folder, frames_folder)\n    os.makedirs(frames_path, exist_ok=True)\n\n    return frames_path\n\ndef extract_frames(video_path, output_folder, frame_interval=1):\n    \"\"\"\n    Extract frames from object-detected video\n    Args:\n        video_path: path to object-detected video\n        output_folder: where to save the frames\n        frame_interval: extract every nth frame\n    \"\"\"\n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error opening video: {video_path}\")\n        return None\n\n    # Get video properties\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    print(f\"Video Properties:\")\n    print(f\"Total Frames: {total_frames}\")\n    print(f\"FPS: {fps}\")\n\n    frame_count = 0\n    saved_count = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Save frame at specified intervals\n        if frame_count % frame_interval == 0:\n            frame_filename = f'frame_{saved_count:06d}.jpg'\n            frame_path = os.path.join(output_folder, frame_filename)\n\n            # Save frame\n            cv2.imwrite(frame_path, frame)\n            saved_count += 1\n\n            # Print progress every 100 frames\n            if saved_count % 100 == 0:\n                print(f\"Saved {saved_count} frames ({(frame_count/total_frames)*100:.2f}% complete)\")\n\n        frame_count += 1\n\n    cap.release()\n    print(f\"\\nExtraction Complete!\")\n    print(f\"Saved {saved_count} frames to {output_folder}\")\n\n    return saved_count\n\n# Usage\nif __name__ == \"__main__\":\n    # Replace with your object-detected video path\n    detected_video = \"/kaggle/input/video-object-detected/vid_object_detected.mp4\"\n\n    # Create folders\n    output_folder = create_folder_structure()\n\n    # Extract every 5th frame (adjust as needed)\n    frame_interval = 5\n\n    # Extract frames\n    frame_count = extract_frames(detected_video, output_folder, frame_interval)\n\n    print(f\"\\nTotal frames extracted: {frame_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport os\n\ndef save_player_crops(video_path, output_dir=\"player_crops\"):\n    \"\"\"Process video and save player crops with unique identifiers\"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load model\n    model = YOLO(\"/kaggle/working/Football-Object-Detection/weights/last.pt\")\n    \n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    total_crops = 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        frame_count += 1\n        \n        # Run detection\n        results = model(frame, conf=0.5, verbose=False)[0]\n        \n        # Process each detection\n        boxes = results.boxes\n        for i, box in enumerate(boxes):\n            # Get box coordinates\n            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n            label = int(box.cls.cpu().numpy()[0])\n            \n            # Only process players and goalkeepers (labels 0 and 1)\n            if label in [0, 1]:\n                # Add padding\n                padding = 5\n                x1_pad = max(0, x1 - padding)\n                y1_pad = max(0, y1 - padding)\n                x2_pad = min(frame.shape[1], x2 + padding)\n                y2_pad = min(frame.shape[0], y2 + padding)\n                \n                # Crop player\n                player_crop = frame[y1_pad:y2_pad, x1_pad:x2_pad]\n                \n                # Save crop with unique identifier\n                filename = f\"frame_{frame_count}_player_{i}_{x1}_{y1}.jpg\"\n                cv2.imwrite(os.path.join(output_dir, filename), player_crop)\n                total_crops += 1\n        \n        # Progress update\n        if frame_count % 100 == 0:\n            print(f\"Processed {frame_count} frames, saved {total_crops} crops\")\n    \n    cap.release()\n    print(f\"Complete: Processed {frame_count} frames, saved {total_crops} crops\")\n    print(f\"Crops saved to {output_dir}\")\n\n# Usage\nvideo_path = \"/kaggle/input/video-object-detected/vid_object_detected.mp4\"\nsave_player_crops(video_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport os\n\ndef process_detected_video(video_path, output_dir=\"player_crops\"):\n    \"\"\"\n    Process video with existing detections and save player crops.\n    Handles both CPU and GPU tensors.\n    \"\"\"\n    # Create output directory if doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Load the model\n    model = YOLO(\"/kaggle/working/Football-Object-Detection/weights/last.pt\")\n    \n    # Open the video\n    cap = cv2.VideoCapture(video_path)\n    frame_count = 0\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        frame_count += 1\n        \n        # Run detection\n        results = model(frame, conf=0.5, verbose=False)[0]\n        \n        # Process each detection\n        boxes = results.boxes\n        for box in boxes:\n            # Get box coordinates - move to CPU first\n            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n            \n            # Get class label - move to CPU first\n            label = int(box.cls.cpu().numpy()[0])\n            \n            # Only process players and goalkeepers (labels 0 and 1)\n            if label in [0, 1]:\n                # Add padding\n                padding = 5\n                x1 = max(0, x1 - padding)\n                y1 = max(0, y1 - padding)\n                x2 = min(frame.shape[1], x2 + padding)\n                y2 = min(frame.shape[0], y2 + padding)\n                \n                # Crop player\n                player_crop = frame[y1:y2, x1:x2]\n                \n                # Save crop with frame number and position\n                crop_filename = f\"frame_{frame_count}_player_{x1}_{y1}.jpg\"\n                cv2.imwrite(os.path.join(output_dir, crop_filename), player_crop)\n        \n        # Optional: Show progress\n        if frame_count % 100 == 0:\n            print(f\"Processed {frame_count} frames\")\n    \n    cap.release()\n    print(f\"Processing complete. Crops saved to {output_dir}\")\n\n# Use it\nvideo_path = \"/kaggle/input/video-object-detected/vid_object_detected.mp4\"\nprocess_detected_video(video_path, \"player_crops\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the content as setup.py\nimport os\nimport gdown\n\n# Create models directory\nos.makedirs('models', exist_ok=True)\n\n# Download soccer legibility model\nsoccer_model_url = \"https://drive.google.com/uc?id=18HAuZbge3z8TSfRiX_FzsnKgiBs-RRNw\"\nsoccer_model_path = \"models/legibility_resnet34_soccer_20240215.pth\"\nif not os.path.exists(soccer_model_path):\n    gdown.download(soccer_model_url, soccer_model_path, fuzzy=True)\n\nprint(\"Downloaded pre-trained legibility model!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef load_legibility_model(model_path):\n    \"\"\"Load pretrained ResNet34 model for legibility classification\"\"\"\n    model = models.resnet34(pretrained=False)\n    num_features = model.fc.in_features\n    model.fc = torch.nn.Linear(num_features, 1)\n    \n    # Load and modify state dict\n    state_dict = torch.load(model_path, map_location='cpu')\n    new_state_dict = {k.replace(\"model_ft.\", \"\"): v for k, v in state_dict.items()}\n    model.load_state_dict(new_state_dict)\n    model.eval()\n    return model\n\ndef process_single_image(model, image_path, transform):\n    \"\"\"Process a single image and return prediction with confidence\"\"\"\n    image = Image.open(image_path).convert('RGB')\n    input_tensor = transform(image).unsqueeze(0)\n    \n    with torch.no_grad():\n        output = model(input_tensor)\n        confidence = torch.sigmoid(output).item()\n        prediction = confidence > 0.5\n    \n    return prediction, confidence, image\n\ndef analyze_and_visualize_crops(model, crops_dir, num_samples=10):\n    \"\"\"Analyze and visualize the legibility classification process\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    results = {'legible': [], 'illegible': []}\n    confidences = []\n    \n    # Process all images\n    for crop_file in os.listdir(crops_dir):\n        if crop_file.endswith('.jpg'):\n            crop_path = os.path.join(crops_dir, crop_file)\n            prediction, confidence, image = process_single_image(model, crop_path, transform)\n            \n            # Store results\n            result = {\n                'path': crop_path,\n                'confidence': confidence,\n                'image': image\n            }\n            \n            if prediction:\n                results['legible'].append(result)\n                # Copy to legible_crops directory\n                os.makedirs('legible_crops', exist_ok=True)\n                new_path = os.path.join('legible_crops', crop_file)\n                os.system(f'cp \"{crop_path}\" \"{new_path}\"')\n            else:\n                results['illegible'].append(result)\n            \n            confidences.append(confidence)\n    \n    # Print statistics\n    total_images = len(confidences)\n    legible_count = len(results['legible'])\n    \n    print(\"\\nLegibility Classification Results:\")\n    print(f\"Total images processed: {total_images}\")\n    print(f\"Legible images: {legible_count} ({legible_count/total_images*100:.1f}%)\")\n    print(f\"Illegible images: {total_images - legible_count} ({(total_images-legible_count)/total_images*100:.1f}%)\")\n    \n    print(\"\\nConfidence Statistics:\")\n    print(f\"Average confidence: {np.mean(confidences):.3f}\")\n    print(f\"Median confidence: {np.median(confidences):.3f}\")\n    print(f\"Standard deviation: {np.std(confidences):.3f}\")\n    \n    # Visualize sample results\n    plt.figure(figsize=(15, 10))\n    \n    # Plot legible samples\n    for i, result in enumerate(results['legible'][:num_samples//2]):\n        plt.subplot(2, num_samples//2, i+1)\n        plt.imshow(result['image'])\n        plt.title(f'Legible\\nConf: {result[\"confidence\"]:.2f}')\n        plt.axis('off')\n    \n    # Plot illegible samples\n    for i, result in enumerate(results['illegible'][:num_samples//2]):\n        plt.subplot(2, num_samples//2, i+1+num_samples//2)\n        plt.imshow(result['image'])\n        plt.title(f'Illegible\\nConf: {result[\"confidence\"]:.2f}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('legibility_analysis.png')\n    plt.close()\n    \n    # Plot confidence distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(confidences, bins=50, edgecolor='black')\n    plt.title('Distribution of Confidence Scores')\n    plt.xlabel('Confidence Score')\n    plt.ylabel('Number of Images')\n    plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Threshold')\n    plt.legend()\n    plt.savefig('confidence_distribution.png')\n    plt.close()\n    \n    return results\n\n# Usage\nmodel_path = \"models/legibility_resnet34_soccer_20240215.pth\"\nmodel = load_legibility_model(model_path)\nresults = analyze_and_visualize_crops(model, \"player_crops\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### After setting up the legitibility classifier, using it for classification","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\ndef load_legibility_model(model_path):\n    # Create ResNet34 model\n    model = models.resnet34(pretrained=False)\n    num_features = model.fc.in_features\n    # Change to match their architecture - output size of 1 instead of 2\n    model.fc = torch.nn.Linear(num_features, 1)  # Single output for binary classification\n\n    # Load state dict and remove 'model_ft' prefix\n    state_dict = torch.load(model_path, map_location='cpu')\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        # Remove 'model_ft.' prefix from the key names\n        name = k.replace(\"model_ft.\", \"\")\n        new_state_dict[name] = v\n\n    # Load the modified state dict\n    model.load_state_dict(new_state_dict)\n    model.eval()\n    return model\n\ndef classify_crops(model, crops_dir):\n    # Image transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n\n    legible_crops = []\n\n    # Process each crop\n    for crop_file in os.listdir(crops_dir):\n        if crop_file.endswith('.jpg'):\n            crop_path = os.path.join(crops_dir, crop_file)\n\n            # Load and transform image\n            image = Image.open(crop_path).convert('RGB')\n            input_tensor = transform(image).unsqueeze(0)\n\n            # Get prediction\n            with torch.no_grad():\n                output = model(input_tensor)\n                # Use sigmoid for binary classification\n                prediction = torch.sigmoid(output) > 0.5\n\n            if prediction.item():  # If legible\n                legible_crops.append(crop_path)\n\n                # Move legible images to a separate directory\n                os.makedirs('legible_crops', exist_ok=True)\n                new_path = os.path.join('legible_crops', crop_file)\n                os.system(f'cp \"{crop_path}\" \"{new_path}\"')\n\n    print(f\"Found {len(legible_crops)} legible crops out of {len(os.listdir(crops_dir))} total crops\")\n    return legible_crops\n\n# Use the models\nmodel_path = \"models/legibility_resnet34_soccer_20240215.pth\"\nmodel = load_legibility_model(model_path)\nlegible_crops = classify_crops(model, \"player_crops\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ViTPose Algorithm for Torso detection","metadata":{}},{"cell_type":"code","source":"import os\nimport gdown\n\n# Create checkpoints directory\nos.makedirs('./ViTPose/checkpoints', exist_ok=True)\n\n# Download ViTPose-H model\nmodel_url = 'https://drive.google.com/uc?id=1A3ftF118IcxMn_QONndR-8dPWpf7XzdV'\nmodel_path = './ViTPose/checkpoints/vitpose-h.pth'\nif not os.path.exists(model_path):\n    print(\"Downloading ViTPose model...\")\n    gdown.download(model_url, model_path, fuzzy=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print versions\nimport mmcv\nimport mmengine\nimport mmdet\nimport mmpose\nimport torch\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"MMCV version:\", mmcv.__version__)\nprint(\"MMEngine version:\", mmengine.__version__)\nprint(\"MMDet version:\", mmdet.__version__)\nprint(\"MMPose version:\", mmpose.__version__)\n\n# Check if model and config exist\nprint(\"\\nChecking files:\")\nprint(\"Model exists:\", os.path.exists('./ViTPose/checkpoints/vitpose-h.pth'))\nprint(\"\\nAvailable configs:\")\nconfig_dir = './ViTPose/configs'\nif os.path.exists(config_dir):\n    for root, dirs, files in os.walk(config_dir):\n        for file in files:\n            if 'vitpose_huge' in file.lower() and file.endswith('.py'):\n                print(os.path.join(root, file))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U openmim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mim download mmpose --config rtmpose-l_8xb256-420e_coco-256x192 --dest .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Check current directory for downloaded files\nprint(\"Files in current directory:\")\n!ls *.py  # Look for config files\n!ls *.pth  # Look for model weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print MMPose version and path\nimport mmpose\nprint(\"MMPose version:\", mmpose.__version__)\nprint(\"MMPose installation path:\", mmpose.__path__)\n\n# Check for RTMPose configs\ndef find_rtmpose_configs(base_path):\n    \"\"\"Find RTMPose config files\"\"\"\n    print(\"\\nSearching for RTMPose configs...\")\n    for root, dirs, files in os.walk(base_path[0]):\n        for file in files:\n            if 'rtmpose' in file.lower() and file.endswith('.py'):\n                print(os.path.join(root, file))\n\nfind_rtmpose_configs(mmpose.__path__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom mmpose.apis import inference_topdown, init_model\nfrom mmpose.utils import register_all_modules\nimport matplotlib.pyplot as plt\n\ndef visualize_vitpose_steps(image_path, model):\n    \"\"\"Visualize each step of ViTPose processing\"\"\"\n    \n    # 1. Load and preprocess image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    print(\"Input Image Shape:\", image.shape)\n    \n    # 2. Get pose estimation results\n    bboxes = [[0, 0, image.shape[1], image.shape[0]]]\n    results = inference_topdown(model, image, bboxes)\n    \n    # Extract keypoints from the new format\n    # The keypoints are now in pred_instances\n    keypoints = results[0].pred_instances.keypoints[0]  # Shape: (17, 2)\n    scores = results[0].pred_instances.keypoint_scores[0]  # Shape: (17,)\n    \n    # Combine keypoints and scores\n    keypoints_with_scores = np.concatenate([keypoints, scores[:, None]], axis=1)\n    print(\"\\nKeypoints Shape:\", keypoints_with_scores.shape)\n    print(\"\\nSample Keypoint (Left Shoulder):\", keypoints_with_scores[5])\n    \n    # 3. Visualize keypoints on image\n    def draw_keypoints(img, keypoints_with_scores):\n        img_copy = img.copy()\n        for i, (x, y, conf) in enumerate(keypoints_with_scores):\n            if conf > 0.3:  # Only draw high-confidence points\n                cv2.circle(img_copy, (int(x), int(y)), 3, (0, 255, 0), -1)\n                cv2.putText(img_copy, str(i), (int(x), int(y)), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n        return img_copy\n    \n    keypoint_viz = draw_keypoints(image_rgb, keypoints_with_scores)\n    \n    # 4. Extract and visualize torso\n    def get_torso_crop(img, keypoints_with_scores):\n        # Get torso keypoints\n        l_shoulder = keypoints_with_scores[5][:2]\n        r_shoulder = keypoints_with_scores[6][:2]\n        l_hip = keypoints_with_scores[11][:2]\n        r_hip = keypoints_with_scores[12][:2]\n        \n        # Calculate bounds\n        top = max(0, int(min(l_shoulder[1], r_shoulder[1])))\n        bottom = min(img.shape[0], int(max(l_hip[1], r_hip[1])))\n        left = max(0, int(min(l_shoulder[0], l_hip[0])))\n        right = min(img.shape[1], int(max(r_shoulder[0], r_hip[0])))\n        \n        # Add padding\n        padding = 5\n        top = max(0, top - padding)\n        bottom = min(img.shape[0], bottom + padding)\n        left = max(0, left - padding)\n        right = min(img.shape[1], right + padding)\n        \n        # Draw torso box\n        img_with_box = img.copy()\n        cv2.rectangle(img_with_box, (left, top), (right, bottom), (255, 0, 0), 2)\n        \n        # Crop torso\n        torso = img[top:bottom, left:right]\n        \n        return img_with_box, torso\n    \n    boxed_image, torso_crop = get_torso_crop(image_rgb, keypoints_with_scores)\n    \n    # Print keypoint coordinates and confidence\n    print(\"\\nKeypoint Coordinates and Confidence:\")\n    for i, (x, y, conf) in enumerate(keypoints_with_scores):\n        if conf > 0.3:  # Only show high-confidence points\n            print(f\"Keypoint {i}: x={x:.1f}, y={y:.1f}, confidence={conf:.2f}\")\n    \n    # Visualize results\n    plt.figure(figsize=(15, 10))\n    \n    plt.subplot(2, 2, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(image_rgb)\n    \n    plt.subplot(2, 2, 2)\n    plt.title(\"Detected Keypoints\")\n    plt.imshow(keypoint_viz)\n    \n    plt.subplot(2, 2, 3)\n    plt.title(\"Torso Box\")\n    plt.imshow(boxed_image)\n    \n    plt.subplot(2, 2, 4)\n    plt.title(\"Torso Crop\")\n    plt.imshow(torso_crop)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'original': image_rgb,\n        'keypoints': keypoints_with_scores,\n        'keypoint_viz': keypoint_viz,\n        'boxed': boxed_image,\n        'torso': torso_crop\n    }\n\n# Usage\nregister_all_modules()\nconfig = 'rtmpose-l_8xb256-420e_coco-256x192.py'\ncheckpoint = 'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth'\nmodel = init_model(config, checkpoint, device='cpu')\n\n# Process a single image\nimage_path = '/kaggle/working/legible_crops/frame_10_player_0_785_475.jpg'\nresults = visualize_vitpose_steps(image_path, model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mmpose.apis import inference_topdown\nfrom mmpose.apis import init_model\nfrom mmpose.utils import register_all_modules\nimport cv2\nimport numpy as np\n\n# Register all modules (This is important!)\nregister_all_modules()\n\ndef test_mmpose():\n    # Model loading\n    config = 'rtmpose-l_8xb256-420e_coco-256x192.py'  # Use the config we downloaded\n    checkpoint = 'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth'  # Use the checkpoint we downloaded\n    model = init_model(config, checkpoint, device='cpu')\n    \n    # Load an image\n    img = cv2.imread('legible_crops/frame_703_player_460_251.jpg')\n    \n    # Single instance detection\n    bboxes = [[0, 0, img.shape[1], img.shape[0]]]  # Full image bbox\n    \n    print(\"Model:\", type(model))\n    print(\"Image shape:\", img.shape)\n    print(\"Bboxes:\", bboxes)\n    \n    # Try inference\n    try:\n        results = inference_topdown(model, img, bboxes)\n        print(\"Results:\", results)\n    except Exception as e:\n        print(\"Error:\", e)\n        print(\"Model config:\", model.cfg)\n\ntest_mmpose()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom mmpose.apis import inference_topdown, init_model\nfrom mmpose.utils import register_all_modules\nimport matplotlib.pyplot as plt\n\ndef visualize_vitpose_steps(image_path, model):\n    \"\"\"Visualize each step of ViTPose processing\"\"\"\n    \n    # 1. Load and preprocess image\n    image = cv2.imread(image_path)\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    print(\"Input Image Shape:\", image.shape)\n    \n    # 2. Get pose estimation results\n    bboxes = [[0, 0, image.shape[1], image.shape[0]]]\n    results = inference_topdown(model, image, bboxes)\n    keypoints = results[0].keypoints[0]  # Shape: (17, 3)\n    print(\"\\nKeypoints Shape:\", keypoints.shape)\n    print(\"\\nSample Keypoint (Left Shoulder):\", keypoints[5])\n    \n    # 3. Visualize keypoints on image\n    def draw_keypoints(img, keypoints):\n        img_copy = img.copy()\n        for i, (x, y, conf) in enumerate(keypoints):\n            if conf > 0.3:  # Only draw high-confidence points\n                cv2.circle(img_copy, (int(x), int(y)), 3, (0, 255, 0), -1)\n                cv2.putText(img_copy, str(i), (int(x), int(y)), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n        return img_copy\n    \n    keypoint_viz = draw_keypoints(image_rgb, keypoints)\n    \n    # 4. Extract and visualize torso\n    def get_torso_crop(img, keypoints):\n        # Get torso keypoints\n        l_shoulder = keypoints[5][:2]\n        r_shoulder = keypoints[6][:2]\n        l_hip = keypoints[11][:2]\n        r_hip = keypoints[12][:2]\n        \n        # Calculate bounds\n        top = max(0, int(min(l_shoulder[1], r_shoulder[1])))\n        bottom = min(img.shape[0], int(max(l_hip[1], r_hip[1])))\n        left = max(0, int(min(l_shoulder[0], l_hip[0])))\n        right = min(img.shape[1], int(max(r_shoulder[0], r_hip[0])))\n        \n        # Add padding\n        padding = 5\n        top = max(0, top - padding)\n        bottom = min(img.shape[0], bottom + padding)\n        left = max(0, left - padding)\n        right = min(img.shape[1], right + padding)\n        \n        # Draw torso box\n        img_with_box = img.copy()\n        cv2.rectangle(img_with_box, (left, top), (right, bottom), (255, 0, 0), 2)\n        \n        # Crop torso\n        torso = img[top:bottom, left:right]\n        \n        return img_with_box, torso\n    \n    boxed_image, torso_crop = get_torso_crop(image_rgb, keypoints)\n    \n    # Print keypoint coordinates and confidence\n    print(\"\\nKeypoint Coordinates and Confidence:\")\n    for i, (x, y, conf) in enumerate(keypoints):\n        if conf > 0.3:  # Only show high-confidence points\n            print(f\"Keypoint {i}: x={x:.1f}, y={y:.1f}, confidence={conf:.2f}\")\n    \n    # Visualize results\n    plt.figure(figsize=(15, 10))\n    \n    plt.subplot(2, 2, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(image_rgb)\n    \n    plt.subplot(2, 2, 2)\n    plt.title(\"Detected Keypoints\")\n    plt.imshow(keypoint_viz)\n    \n    plt.subplot(2, 2, 3)\n    plt.title(\"Torso Box\")\n    plt.imshow(boxed_image)\n    \n    plt.subplot(2, 2, 4)\n    plt.title(\"Torso Crop\")\n    plt.imshow(torso_crop)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return {\n        'original': image_rgb,\n        'keypoints': keypoints,\n        'keypoint_viz': keypoint_viz,\n        'boxed': boxed_image,\n        'torso': torso_crop\n    }\n\n# Usage\nconfig = 'rtmpose-l_8xb256-420e_coco-256x192.py'\ncheckpoint = 'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth'\nmodel = init_model(config, checkpoint, device='cpu')\n\n# Process a single image\nimage_path = 'path_to_your_player_crop.jpg'\nresults = visualize_vitpose_steps(image_path, model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_pose_results(image_path, results):\n    \"\"\"Visualize the pose estimation results\"\"\"\n    # Read image\n    img = cv2.imread(image_path)\n    vis_img = img.copy()\n    \n    # Get keypoints\n    keypoints = results[0].pred_instances.keypoints[0]\n    scores = results[0].pred_instances.keypoint_scores[0]\n    \n    # Points we're interested in\n    points_of_interest = {\n        5: ('Left Shoulder', (0, 0, 255)),    # Red\n        6: ('Right Shoulder', (0, 255, 0)),   # Green\n        11: ('Left Hip', (255, 0, 0)),        # Blue\n        12: ('Right Hip', (255, 255, 0))      # Cyan\n    }\n    \n    # Draw keypoints\n    for idx, (name, color) in points_of_interest.items():\n        x, y = map(int, keypoints[idx])\n        conf = scores[idx]\n        \n        # Draw point and label\n        cv2.circle(vis_img, (x, y), 3, color, -1)\n        cv2.putText(vis_img, f\"{name}: {conf:.2f}\", \n                   (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, \n                   0.3, color, 1)\n    \n    # Draw torso region\n    pts = np.array([\n        keypoints[5][:2],  # Left shoulder\n        keypoints[6][:2],  # Right shoulder\n        keypoints[12][:2], # Right hip\n        keypoints[11][:2]  # Left hip\n    ], np.int32)\n    \n    cv2.polylines(vis_img, [pts], True, (255, 255, 255), 1)\n    \n    # Display results\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))\n    plt.title('Detected Pose')\n    plt.axis('off')\n    \n    plt.show()\n\n# Run the visualization\nimage_path = 'legible_crops/frame_703_player_460_251.jpg'\nmodel = init_model('rtmpose-l_8xb256-420e_coco-256x192.py', \n                  'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth', \n                  device='cpu')\nresults = inference_topdown(model, cv2.imread(image_path), [[0, 0, 59, 89]])\nvisualize_pose_results(image_path, results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_torso_region(img, results):\n   \"\"\"\n   Extract torso region using pose keypoints\n   Args:\n       img: Original image\n       results: Pose estimation results from MMPose\n   Returns:\n       Cropped torso image\n   \"\"\"\n   # Get keypoints\n   keypoints = results[0].pred_instances.keypoints[0]\n   \n   # Extract relevant keypoints\n   left_shoulder = keypoints[5][:2].astype(int)\n   right_shoulder = keypoints[6][:2].astype(int)\n   left_hip = keypoints[11][:2].astype(int)\n   right_hip = keypoints[12][:2].astype(int)\n   \n   # Calculate bounding box with padding\n   padding = 5\n   x1 = max(0, int(min(left_shoulder[0], left_hip[0])) - padding)\n   y1 = max(0, int(min(left_shoulder[1], right_shoulder[1])) - padding)\n   x2 = min(img.shape[1], int(max(right_shoulder[0], right_hip[0])) + padding)\n   y2 = min(img.shape[0], int(max(left_hip[1], right_hip[1])) + padding)\n   \n   # Crop torso region\n   torso_crop = img[y1:y2, x1:x2]\n   \n   return torso_crop, (x1, y1, x2, y2)\n\n# Test the torso extraction\ndef test_torso_extraction():\n   # Initialize model and get pose results (using our previous code)\n   image_path = 'legible_crops/frame_703_player_460_251.jpg'\n   model = init_model('rtmpose-l_8xb256-420e_coco-256x192.py', \n                     'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth', \n                     device='cpu')\n   img = cv2.imread(image_path)\n   results = inference_topdown(model, img, [[0, 0, img.shape[1], img.shape[0]]])\n   \n   # Extract torso region\n   torso_crop, bbox = extract_torso_region(img, results)\n   \n   # Visualize results\n   plt.figure(figsize=(15, 5))\n   \n   # Original image with keypoints and torso region\n   vis_img = img.copy()\n   x1, y1, x2, y2 = bbox\n   \n   # Draw keypoints\n   keypoints = results[0].pred_instances.keypoints[0]\n   keypoint_names = {\n       5: ('Left Shoulder', (0, 0, 255)),    # Red\n       6: ('Right Shoulder', (0, 255, 0)),   # Green\n       11: ('Left Hip', (255, 0, 0)),        # Blue\n       12: ('Right Hip', (255, 255, 0))      # Cyan\n   }\n   \n   for idx, (name, color) in keypoint_names.items():\n       x, y = map(int, keypoints[idx][:2])\n       cv2.circle(vis_img, (x, y), 3, color, -1)\n   \n   # Draw torso bounding box\n   cv2.rectangle(vis_img, (x1, y1), (x2, y2), (255, 255, 255), 2)\n   \n   # Show original with detected region\n   plt.subplot(1, 3, 1)\n   plt.imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))\n   plt.title('Detected Torso Region')\n   plt.axis('off')\n   \n   # Show cropped torso\n   plt.subplot(1, 3, 2)\n   plt.imshow(cv2.cvtColor(torso_crop, cv2.COLOR_BGR2RGB))\n   plt.title('Cropped Torso')\n   plt.axis('off')\n   \n   plt.show()\n   \n   return torso_crop\n\n# Run the test\ntorso_crop = test_torso_extraction()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_torso_region(img, results, padding=5, apply_preprocessing=True):\n    \"\"\"\n    Extract torso region with optional preprocessing\n    \"\"\"\n    # Get keypoints\n    keypoints = results[0].pred_instances.keypoints[0]\n    \n    # Extract relevant keypoints\n    left_shoulder = keypoints[5][:2].astype(int)\n    right_shoulder = keypoints[6][:2].astype(int)\n    left_hip = keypoints[11][:2].astype(int)\n    right_hip = keypoints[12][:2].astype(int)\n    \n    # Calculate bounding box with padding\n    x1 = max(0, int(min(left_shoulder[0], left_hip[0])) - padding)\n    y1 = max(0, int(min(left_shoulder[1], right_shoulder[1])) - padding)\n    x2 = min(img.shape[1], int(max(right_shoulder[0], right_hip[0])) + padding)\n    y2 = min(img.shape[0], int(max(left_hip[1], right_hip[1])) + padding)\n    \n    # Crop torso region\n    torso_crop = img[y1:y2, x1:x2]\n    \n    if apply_preprocessing:\n        # Apply some basic preprocessing\n        # 1. Resize to larger size\n        torso_crop = cv2.resize(torso_crop, (0,0), fx=2, fy=2)\n        \n        # 2. Sharpen the image\n        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n        torso_crop = cv2.filter2D(torso_crop, -1, kernel)\n        \n        # 3. Enhance contrast\n        lab = cv2.cvtColor(torso_crop, cv2.COLOR_BGR2LAB)\n        l, a, b = cv2.split(lab)\n        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n        cl = clahe.apply(l)\n        limg = cv2.merge((cl,a,b))\n        torso_crop = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n    \n    return torso_crop, (x1, y1, x2, y2)\n\n# Test with different parameters\ndef test_torso_extraction_with_preprocessing():\n    image_path = 'legible_crops/frame_703_player_460_251.jpg'\n    model = init_model('rtmpose-l_8xb256-420e_coco-256x192.py', \n                      'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth', \n                      device='cpu')\n    img = cv2.imread(image_path)\n    results = inference_topdown(model, img, [[0, 0, img.shape[1], img.shape[0]]])\n    \n    # Get crops with different processing\n    torso_original, bbox = extract_torso_region(img, results, padding=5, apply_preprocessing=False)\n    torso_processed, _ = extract_torso_region(img, results, padding=5, apply_preprocessing=True)\n    \n    # Visualize results\n    plt.figure(figsize=(15, 5))\n    \n    # Original crop\n    plt.subplot(1, 2, 1)\n    plt.imshow(cv2.cvtColor(torso_original, cv2.COLOR_BGR2RGB))\n    plt.title('Original Crop')\n    plt.axis('off')\n    \n    # Processed crop\n    plt.subplot(1, 2, 2)\n    plt.imshow(cv2.cvtColor(torso_processed, cv2.COLOR_BGR2RGB))\n    plt.title('Processed Crop')\n    plt.axis('off')\n    \n    plt.show()\n    \n    return torso_original, torso_processed\n\n# Run the test\noriginal_crop, processed_crop = test_torso_extraction_with_preprocessing()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom mmpose.apis import inference_topdown, init_model\nfrom mmpose.utils import register_all_modules\nimport matplotlib.pyplot as plt\n\ndef create_output_directories():\n    \"\"\"Create organized directory structure for outputs\"\"\"\n    directories = [\n        'output',\n        'output/original_with_keypoints',\n        'output/torso_crops_original',\n        'output/torso_crops_processed'\n    ]\n    \n    for dir_path in directories:\n        os.makedirs(dir_path, exist_ok=True)\n        print(f\"Created directory: {dir_path}\")\n\ndef process_single_image(model, image_path):\n    \"\"\"Process a single image through ViTPose pipeline\"\"\"\n    # Read image\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Could not read image: {image_path}\")\n        return None\n    \n    # Get filename without extension\n    base_name = os.path.basename(image_path).split('.')[0]\n    \n    try:\n        # Run pose estimation\n        results = inference_topdown(model, img, [[0, 0, img.shape[1], img.shape[0]]])\n        \n        # Visualize keypoints on original image\n        vis_img = img.copy()\n        keypoints = results[0].pred_instances.keypoints[0]\n        \n        # Draw keypoints\n        keypoint_names = {\n            5: ('Left Shoulder', (0, 0, 255)),    # Red\n            6: ('Right Shoulder', (0, 255, 0)),   # Green\n            11: ('Left Hip', (255, 0, 0)),        # Blue\n            12: ('Right Hip', (255, 255, 0))      # Cyan\n        }\n        \n        for idx, (name, color) in keypoint_names.items():\n            x, y = map(int, keypoints[idx][:2])\n            cv2.circle(vis_img, (x, y), 3, color, -1)\n            cv2.putText(vis_img, name, (x-10, y-10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)\n        \n        # Save keypoint visualization\n        keypoint_path = f'output/original_with_keypoints/{base_name}_keypoints.jpg'\n        cv2.imwrite(keypoint_path, vis_img)\n        \n        # Extract and save torso crops\n        torso_original, torso_processed = extract_torso_crops(img, results)\n        \n        if torso_original is not None:\n            # Save original crop\n            crop_path = f'output/torso_crops_original/{base_name}_torso.jpg'\n            cv2.imwrite(crop_path, torso_original)\n            \n            # Save processed crop\n            processed_path = f'output/torso_crops_processed/{base_name}_torso_processed.jpg'\n            cv2.imwrite(processed_path, torso_processed)\n            \n        return True\n        \n    except Exception as e:\n        print(f\"Error processing {image_path}: {str(e)}\")\n        return False\n\ndef extract_torso_crops(img, results):\n    \"\"\"Extract both original and processed torso crops\"\"\"\n    # Get keypoints\n    keypoints = results[0].pred_instances.keypoints[0]\n    \n    # Extract relevant keypoints\n    left_shoulder = keypoints[5][:2].astype(int)\n    right_shoulder = keypoints[6][:2].astype(int)\n    left_hip = keypoints[11][:2].astype(int)\n    right_hip = keypoints[12][:2].astype(int)\n    \n    # Calculate bounding box with padding\n    padding = 5\n    x1 = max(0, int(min(left_shoulder[0], left_hip[0])) - padding)\n    y1 = max(0, int(min(left_shoulder[1], right_shoulder[1])) - padding)\n    x2 = min(img.shape[1], int(max(right_shoulder[0], right_hip[0])) + padding)\n    y2 = min(img.shape[0], int(max(left_hip[1], right_hip[1])) + padding)\n    \n    # Get original crop\n    torso_original = img[y1:y2, x1:x2]\n    \n    # Create processed crop\n    torso_processed = torso_original.copy()\n    \n    # Apply preprocessing\n    # 1. Resize\n    torso_processed = cv2.resize(torso_processed, (0,0), fx=2, fy=2)\n    \n    # 2. Sharpen\n    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n    torso_processed = cv2.filter2D(torso_processed, -1, kernel)\n    \n    # 3. Enhance contrast\n    lab = cv2.cvtColor(torso_processed, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl,a,b))\n    torso_processed = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n    \n    return torso_original, torso_processed\n\ndef process_all_legible_crops():\n    \"\"\"Process all images in the legible_crops directory\"\"\"\n    # Create output directories\n    create_output_directories()\n    \n    # Initialize model\n    print(\"Initializing ViTPose model...\")\n    model = init_model('rtmpose-l_8xb256-420e_coco-256x192.py', \n                      'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth', \n                      device='cpu')\n    \n    # Process each image\n    legible_crops_dir = 'legible_crops'\n    total_images = 0\n    successful_processes = 0\n    \n    for filename in os.listdir(legible_crops_dir):\n        if filename.endswith(('.jpg', '.jpeg', '.png')):\n            total_images += 1\n            image_path = os.path.join(legible_crops_dir, filename)\n            print(f\"\\nProcessing {filename}...\")\n            \n            if process_single_image(model, image_path):\n                successful_processes += 1\n    \n    print(f\"\\nProcessing complete!\")\n    print(f\"Total images processed: {total_images}\")\n    print(f\"Successful processes: {successful_processes}\")\n    print(f\"Failed processes: {total_images - successful_processes}\")\n\n# Run the processing\nregister_all_modules()\nprocess_all_legible_crops()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Post processing + Attempting to identify the number \n(Still a work in progress)","metadata":{}},{"cell_type":"code","source":"def analyze_failed_image(model, image_name):\n    \"\"\"Detailed analysis of a failed image with better error handling\"\"\"\n    print(f\"\\nAnalyzing: {image_name}\")\n    image_path = os.path.join('legible_crops', image_name)\n    \n    # Read image\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Could not read image: {image_path}\")\n        return\n    \n    print(f\"Image shape: {img.shape}\")\n    \n    try:\n        # Run pose estimation\n        results = inference_topdown(model, img, [[0, 0, img.shape[1], img.shape[0]]])\n        \n        # Debug print of results structure\n        print(\"\\nResults structure:\")\n        print(f\"Results: {results}\")\n        print(f\"Length of results: {len(results)}\")\n        if len(results) > 0:\n            print(f\"First result attributes: {dir(results[0])}\")\n            if hasattr(results[0], 'pred_instances'):\n                print(f\"Keypoints shape: {results[0].pred_instances.keypoints.shape}\")\n        \n        # Save original image for reference\n        os.makedirs('debug_output', exist_ok=True)\n        cv2.imwrite(f'debug_output/original_{image_name}', img)\n        \n    except Exception as e:\n        print(f\"Error during analysis:\")\n        print(f\"Error type: {type(e)}\")\n        print(f\"Error message: {str(e)}\")\n\n# Initialize model and run analysis\nregister_all_modules()\nmodel = init_model('rtmpose-l_8xb256-420e_coco-256x192.py', \n                  'rtmpose-l_simcc-coco_pt-aic-coco_420e-256x192-1352a4d2_20230127.pth', \n                  device='cuda' if torch.cuda.is_available() else 'cpu')\n\n# Now pass both model and image_name\nanalyze_failed_image(model, 'frame_528_player_458_292.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_failed_image(model, image_name):\n    \"\"\"Detailed analysis of a failed image with visualization\"\"\"\n    print(f\"\\nAnalyzing: {image_name}\")\n    image_path = os.path.join('legible_crops', image_name)\n    \n    # Read image\n    img = cv2.imread(image_path)\n    if img is None:\n        print(f\"Could not read image: {image_path}\")\n        return\n    \n    print(f\"Image shape: {img.shape}\")\n    \n    try:\n        # Run pose estimation\n        results = inference_topdown(model, img, [[0, 0, img.shape[1], img.shape[0]]])\n        \n        # Get keypoints and scores\n        keypoints = results[0].pred_instances.keypoints[0]  # Shape: (17, 2)\n        scores = results[0].pred_instances.keypoint_scores[0]  # Shape: (17,)\n        \n        # Create debug visualization\n        debug_img = img.copy()\n        \n        # Points we're interested in\n        key_points = {\n            5: ('Left Shoulder', (0, 0, 255)),    # Red\n            6: ('Right Shoulder', (0, 255, 0)),   # Green\n            11: ('Left Hip', (255, 0, 0)),        # Blue\n            12: ('Right Hip', (255, 255, 0))      # Cyan\n        }\n        \n        print(\"\\nKeypoint Coordinates and Confidence:\")\n        for idx, (name, color) in key_points.items():\n            x, y = keypoints[idx]\n            conf = scores[idx]\n            print(f\"{name}: ({x:.2f}, {y:.2f}), confidence: {conf:.2f}\")\n            \n            # Draw on debug image\n            x, y = int(x), int(y)\n            cv2.circle(debug_img, (x, y), 3, color, -1)\n            cv2.putText(debug_img, f\"{name}: {conf:.2f}\", \n                       (x-10, y-10), cv2.FONT_HERSHEY_SIMPLEX, \n                       0.3, color, 1)\n        \n        # Calculate and draw potential crop region\n        padding = 5\n        x1 = max(0, int(min(keypoints[5][0], keypoints[11][0])) - padding)\n        y1 = max(0, int(min(keypoints[5][1], keypoints[6][1])) - padding)\n        x2 = min(img.shape[1], int(max(keypoints[6][0], keypoints[12][0])) + padding)\n        y2 = min(img.shape[0], int(max(keypoints[11][1], keypoints[12][1])) + padding)\n        \n        print(\"\\nProposed crop dimensions:\")\n        print(f\"x1: {x1}, x2: {x2}, width: {x2-x1}\")\n        print(f\"y1: {y1}, y2: {y2}, height: {y2-y1}\")\n        \n        # Draw crop region\n        cv2.rectangle(debug_img, (x1, y1), (x2, y2), (255, 255, 255), 1)\n        \n        # Save debug visualization\n        os.makedirs('debug_output', exist_ok=True)\n        cv2.imwrite(f'debug_output/debug_{image_name}', debug_img)\n        print(f\"\\nSaved debug visualization to debug_output/debug_{image_name}\")\n        \n    except Exception as e:\n        print(f\"Error during analysis:\")\n        print(f\"Error type: {type(e)}\")\n        print(f\"Error message: {str(e)}\")\n\n# Run the analysis\nanalyze_failed_image(model, 'frame_528_player_458_292.jpg')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r folder_name.zip /kaggle/working/output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Using ParSeq to identify the jersey number using ViTPose output","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/baudm/parseq.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Change to parseq directory\n%cd parseq\n\n# Install dependencies\n!pip install -r requirements/core.txt\n!pip install -e .\n\n# Return to original directory\n%cd ..","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gdown\n\n# Create models directory\nos.makedirs('models', exist_ok=True)\n\n# Download PARSeq model for jersey numbers\nmodel_url = \"https://drive.google.com/uc?id=1uRln22tlhneVt3P6MePmVxBWSLMsL3bm\"\nmodel_path = \"models/parseq_epoch=24-step=2575-val_accuracy=95.6044-val_NED=96.3255.ckpt\"\ngdown.download(model_url, model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Try importing required modules\ntry:\n    from strhub.models.parseq.model import PARSeq\n    from strhub.models.utils import load_from_checkpoint\n    print(\"PARSeq successfully installed!\")\n    \n    # Try loading the model\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = load_from_checkpoint(model_path, device=device)\n    print(\"Model loaded successfully!\")\n    \nexcept Exception as e:\n    print(f\"Error during verification: {str(e)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install timm==0.4.9\n!pip install einops\n!pip install pytorch-lightning","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd jersey-number-pipeline/str/parseq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r requirements/core.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conda env list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo find . -name \"*torch*\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import strhub.models.parseq.system","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Uninstall current PyTorch\n# !pip uninstall -y torch torchvision torchaudio\n\n# # Install compatible PyTorch version\n# !pip install torch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0\n\n# # Verify PyTorch version\n# import torch\n# print(\"PyTorch version:\", torch.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport torch\nfrom strhub.models.parseq.model import PARSeq\nfrom strhub.models.utils import load_from_checkpoint\nfrom torchvision import transforms\nfrom PIL import Image\n\ndef recognize_jersey_number(torso_image):\n    \"\"\"\n    Recognize jersey number from torso crop using PARSeq\n    Args:\n        torso_image: CV2 image (numpy array) of cropped torso region\n    Returns:\n        predicted number as string\n    \"\"\"\n    # Load PARSeq model\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = load_from_checkpoint(\n        \"models/parseq_epoch=24-step=2575-val_accuracy=95.6044-val_NED=96.3255.ckpt\",\n        device=device\n    )\n    model.eval()\n    \n    # Prepare image\n    transform = transforms.Compose([\n        transforms.Resize((32, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n    \n    # Convert CV2 image to PIL\n    pil_image = Image.fromarray(cv2.cvtColor(torso_image, cv2.COLOR_BGR2RGB))\n    \n    # Process image\n    with torch.no_grad():\n        img = transform(pil_image).unsqueeze(0).to(device)\n        logits = model(img)\n        pred = model.tokenizer.decode(logits)\n    \n    return pred[0]\n\n# Test function to process a directory of torso crops\ndef process_torso_crops(crops_dir='torso_crops'):\n    \"\"\"Process all torso crops and recognize jersey numbers\"\"\"\n    results = {}\n    \n    for filename in os.listdir(crops_dir):\n        if filename.endswith(('.jpg', '.jpeg', '.png')):\n            image_path = os.path.join(crops_dir, filename)\n            \n            # Read image\n            torso_image = cv2.imread(image_path)\n            \n            try:\n                # Recognize number\n                number = recognize_jersey_number(torso_image)\n                results[filename] = number\n                print(f\"{filename}: Detected number {number}\")\n                \n                # Save visualization\n                vis_img = torso_image.copy()\n                cv2.putText(vis_img, f\"Number: {number}\", \n                           (10, 20), cv2.FONT_HERSHEY_SIMPLEX, \n                           0.5, (0, 255, 0), 1)\n                \n                os.makedirs('number_recognition_results', exist_ok=True)\n                cv2.imwrite(f'number_recognition_results/numbered_{filename}', vis_img)\n                \n            except Exception as e:\n                print(f\"Error processing {filename}: {str(e)}\")\n    \n    return results\n\n# Run the processing\nresults = process_torso_crops()\nprint(\"\\nProcessing complete!\")\nprint(f\"Total images processed: {len(results)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}